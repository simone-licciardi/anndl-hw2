% Reference for Mars terrain semantic segmentation
@misc{MarsSeg,
      title={MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector}, 
      author={Junbo Li, Keyan Chen, Gengju Tian, Lu Li, Zhenwei Shi},
      year={2024},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/html/2404.04155v1}, 
}

% Reference for the whole project
@article{islam2023explainable,
    author = {Islam, Oahidul and Assaduzzaman, Md and Hasan, Md Zahid},
    title = {An explainable AI-based blood cell classification using optimized convolutional neural network},
    journal = {Department of EEE and HIRL, Daffodil International University, Bangladesh},
    year = {2023},
    note = {Available online},
}

% Reference for Hyperparameter Tunig Strategy 
@inproceedings{hossain2021machine,
    author = {Hossain, Md Riyad and Timmer, Douglas and Moya, Hiram},
    title = {Machine learning model optimization with hyper-parameter tuning approach},
    booktitle = {Proceedings of the International Conference on Advanced Computing},
    year = {2021},
    month = {August},
}

@misc{mandt2018stochasticgradientdescentapproximate,
      title={Stochastic Gradient Descent as Approximate Bayesian Inference}, 
      author={Stephan Mandt and Matthew D. Hoffman and David M. Blei},
      year={2018},
      eprint={1704.04289},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1704.04289}, 
}

@misc{yun2024stochgradadamacceleratingneuralnetworks,
      title={StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling}, 
      author={Juyoung Yun},
      year={2024},
      eprint={2310.17042},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17042}, 
}

% Reference for Hyperparameter Tunig Strategy 
@article{chen2023,
    author = {Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le},
    title = {Symbolic Discovery of Optimization Algorithms},
    year = {2023},
    month = {May},
    note = {Available online}
}


@Inbook{LeCun1998,
author="LeCun, Yann
and Bottou, Leon
and Orr, Genevieve B.
and M{\"u}ller, Klaus -Robert",
editor="Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--50",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
isbn="978-3-540-49430-0",
doi="10.1007/3-540-49430-8_2",
url="https://doi.org/10.1007/3-540-49430-8_2"
}


